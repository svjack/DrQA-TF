{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file load from ../notebook_dumps/args.json\n",
      "file load from ../notebook_dumps/word_dict.pkl\n",
      "file load from ../notebook_dumps/feature_dict.pkl\n",
      "file load from ../notebook_dumps/dev_exs.pkl\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import pickle as pkl\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "def vectorize_np(ex, args, word_dict, feature_dict, single_answer=False):\n",
    "    \"\"\"Torchify a single example.\"\"\"\n",
    "    #args = model.args\n",
    "    #word_dict = model.word_dict\n",
    "    #feature_dict = model.feature_dict\n",
    "\n",
    "    # Index words\n",
    "    #document = torch.LongTensor([word_dict[w] for w in ex['document']])\n",
    "    #question = torch.LongTensor([word_dict[w] for w in ex['question']])\n",
    "    from collections import Counter\n",
    "    document = np.asarray([word_dict[w] for w in ex['document']], dtype = np.int32)\n",
    "    question = np.asarray([word_dict[w] for w in ex['question']], dtype = np.int32)\n",
    "\n",
    "    # Create extra features vector\n",
    "    if len(feature_dict) > 0:\n",
    "        #features = torch.zeros(len(ex['document']), len(feature_dict))\n",
    "        features = np.zeros([len(ex['document']), len(feature_dict)])\n",
    "    else:\n",
    "        features = None\n",
    "\n",
    "    # f_{exact_match}\n",
    "    if args.use_in_question:\n",
    "        q_words_cased = {w for w in ex['question']}\n",
    "        q_words_uncased = {w.lower() for w in ex['question']}\n",
    "        q_lemma = {w for w in ex['qlemma']} if args.use_lemma else None\n",
    "        for i in range(len(ex['document'])):\n",
    "            if ex['document'][i] in q_words_cased:\n",
    "                features[i][feature_dict['in_question']] = 1.0\n",
    "            if ex['document'][i].lower() in q_words_uncased:\n",
    "                features[i][feature_dict['in_question_uncased']] = 1.0\n",
    "            if q_lemma and ex['lemma'][i] in q_lemma:\n",
    "                features[i][feature_dict['in_question_lemma']] = 1.0\n",
    "\n",
    "    # f_{token} (POS)\n",
    "    if args.use_pos:\n",
    "        for i, w in enumerate(ex['pos']):\n",
    "            f = 'pos=%s' % w\n",
    "            if f in feature_dict:\n",
    "                features[i][feature_dict[f]] = 1.0\n",
    "\n",
    "    # f_{token} (NER)\n",
    "    if args.use_ner:\n",
    "        for i, w in enumerate(ex['ner']):\n",
    "            f = 'ner=%s' % w\n",
    "            if f in feature_dict:\n",
    "                features[i][feature_dict[f]] = 1.0\n",
    "\n",
    "    # f_{token} (TF)\n",
    "    if args.use_tf:\n",
    "        counter = Counter([w.lower() for w in ex['document']])\n",
    "        l = len(ex['document'])\n",
    "        for i, w in enumerate(ex['document']):\n",
    "            features[i][feature_dict['tf']] = counter[w.lower()] * 1.0 / l\n",
    "\n",
    "    # Maybe return without target\n",
    "    if 'answers' not in ex:\n",
    "        return document, features, question, ex['id']\n",
    "\n",
    "    # ...or with target(s) (might still be empty if answers is empty)\n",
    "    if single_answer:\n",
    "        assert(len(ex['answers']) > 0)\n",
    "        #start = torch.LongTensor(1).fill_(ex['answers'][0][0])\n",
    "        #end = torch.LongTensor(1).fill_(ex['answers'][0][1])\n",
    "        start = np.asarray([ex['answers'][0][0]], dtype = np.int32)\n",
    "        end = np.asarray([ex['answers'][0][1]], dtype = np.int32)\n",
    "    else:\n",
    "        start = [a[0] for a in ex['answers']]\n",
    "        end = [a[1] for a in ex['answers']]\n",
    "        \n",
    "        ####\n",
    "        from functools import partial\n",
    "        start, end = map(partial(np.asarray, dtype = np.int32), [start, end])\n",
    "\n",
    "    return document, features, question, start, end, ex['id']\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#notebook_dumps_dir = \"/home/svjack/temp_dir/DrQA/DrQA/notebook_dumps/\"\n",
    "#notebook_dumps_dir = \"notebook_dumps\"\n",
    "notebook_dumps_dir = \"../notebook_dumps\"\n",
    "\n",
    "def load_file(file_format, file_name):\n",
    "    assert file_format in [\"json\", \"pkl\"]\n",
    "    file_path = os.path.join(notebook_dumps_dir, \"{}.{}\".format(file_name, file_format))\n",
    "    if file_format == \"json\":\n",
    "        with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "            obj = json.load(f)\n",
    "    elif file_format == \"pkl\":\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            obj = pkl.load(f)\n",
    "    else:\n",
    "        1 / 0\n",
    "    print(\"file load from {}\".format(file_path))\n",
    "    return obj\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "args_dict = load_file(\"json\", \"args\")\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "args_tuple = namedtuple(\"args_tuple\", list(args_dict.keys()))\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "for k, v in args_dict.items():\n",
    "    exec(\"args_tuple.{}={}\".format(k, \"'{}'\".format(v) if type(v) == type(\"\") else v))\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "word_dict = load_file(\"pkl\", \"word_dict\")\n",
    "feature_dict = load_file(\"pkl\", \"feature_dict\")\n",
    "dev_exs = load_file(\"pkl\", \"dev_exs\")\n",
    "train_exs = load_file(\"pkl\", \"train_exs\")\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def batchify_np(batch):\n",
    "    \"\"\"Gather a batch of individual examples into one batch.\"\"\"\n",
    "    NUM_INPUTS = 3\n",
    "    NUM_TARGETS = 2\n",
    "    NUM_EXTRA = 1\n",
    "\n",
    "    ids = [ex[-1] for ex in batch]\n",
    "    docs = [ex[0] for ex in batch]\n",
    "    features = [ex[1] for ex in batch]\n",
    "    questions = [ex[2] for ex in batch]\n",
    "\n",
    "    # Batch documents and features\n",
    "    #max_length = max([d.size(0) for d in docs])\n",
    "    max_length = max([d.size for d in docs])\n",
    "    x1 = np.zeros([len(docs), max_length])\n",
    "    #x1 = torch.LongTensor(len(docs), max_length).zero_()\n",
    "    #x1_mask = torch.ByteTensor(len(docs), max_length).fill_(1)\n",
    "    x1_length = np.asarray([d.size for d in docs], dtype = np.int32)\n",
    "    if features[0] is None:\n",
    "        x1_f = None\n",
    "    else:\n",
    "        #x1_f = torch.zeros(len(docs), max_length, features[0].size(1))\n",
    "        x1_f = np.zeros([len(docs), max_length, features[0].shape[1]])\n",
    "    for i, d in enumerate(docs):\n",
    "        #x1[i, :d.size(0)].copy_(d)\n",
    "        #x1_mask[i, :d.size(0)].fill_(0)\n",
    "        x1[i, :d.shape[0]] = d\n",
    "        if x1_f is not None:\n",
    "            #x1_f[i, :d.size(0)].copy_(features[i])\n",
    "            x1_f[i, :d.shape[0]] = features[i]\n",
    "            \n",
    "    # Batch questions\n",
    "    #max_length = max([q.size(0) for q in questions])\n",
    "    max_length = max([q.size for q in questions])\n",
    "    #x2 = torch.LongTensor(len(questions), max_length).zero_()\n",
    "    x2 = np.zeros([len(questions), max_length])\n",
    "    #x2_mask = torch.ByteTensor(len(questions), max_length).fill_(1)\n",
    "    x2_length = np.asarray([q.size for q in questions], dtype = np.int32)\n",
    "    for i, q in enumerate(questions):\n",
    "        #x2[i, :q.size(0)].copy_(q)\n",
    "        x2[i, :q.shape[0]] = q\n",
    "        #x2_mask[i, :q.size(0)].fill_(0)\n",
    "\n",
    "    # Maybe return without targets\n",
    "    if len(batch[0]) == NUM_INPUTS + NUM_EXTRA:\n",
    "        return x1, x1_f, x1_length, x2, x2_length, ids\n",
    "\n",
    "    elif len(batch[0]) == NUM_INPUTS + NUM_EXTRA + NUM_TARGETS:\n",
    "        # ...Otherwise add targets\n",
    "        '''\n",
    "        if torch.is_tensor(batch[0][3]):\n",
    "            y_s = torch.cat([ex[3] for ex in batch])\n",
    "            y_e = torch.cat([ex[4] for ex in batch])\n",
    "        else:\n",
    "            y_s = [ex[3] for ex in batch]\n",
    "            y_e = [ex[4] for ex in batch]\n",
    "        '''\n",
    "        y_s = np.concatenate([ex[3] for ex in batch])\n",
    "        y_e = np.concatenate([ex[4] for ex in batch])\n",
    "    else:\n",
    "        raise RuntimeError('Incorrect number of inputs per example.')\n",
    "\n",
    "    return x1, x1_f, x1_length, x2, x2_length, y_s, y_e, ids\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def batch_samples_gen(exs, epoch_num = 1, batch_size = 10, args_tuple = args_tuple, word_dict = word_dict,\n",
    "                      feature_dict = feature_dict, single_answer=True):\n",
    "    assert epoch_num >= 1\n",
    "    for epoch in range(epoch_num):\n",
    "        perm_indices_array = np.random.permutation(list(range(len(exs))))\n",
    "        perm_indices_list = perm_indices_array.tolist()\n",
    "        temp_indices = []\n",
    "        while perm_indices_list:\n",
    "            while len(temp_indices) < batch_size and perm_indices_list:\n",
    "                temp_indices.append(perm_indices_list.pop())\n",
    "            \n",
    "            exs_batch = list(map(lambda idx: vectorize_np(exs[idx], args_tuple, word_dict, feature_dict, single_answer=single_answer),\n",
    "                           temp_indices))\n",
    "            x1, x1_f, x1_length, x2, x2_length, y_s, y_e, ids = batchify_np(exs_batch)\n",
    "            yield x1, x1_f, x1_length, x2, x2_length, y_s, y_e\n",
    "            temp_indices = []\n",
    "        print(\"epoch {} end !\".format(epoch))\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Bidirectional, LSTMCell, RNN, LSTM\n",
    "\n",
    "\n",
    "class RnnDocReader_tf(object):\n",
    "    def __init__(self, embeddings, update_embedding = True, num_features = None):\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        self.update_embedding = update_embedding\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        self.optimizer = \"Adam\"\n",
    "        self.clip_grad = 0.5\n",
    "        \n",
    "        self.build_graph()\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self.add_placeholders()\n",
    "        self.lookup_layer_op()\n",
    "        self.RNN_layer_op(use_avg_weight = \"attn\")\n",
    "        self.softmax_pred_op()\n",
    "        self.loss_op()\n",
    "        self.trainstep_op()\n",
    "        self.init_op()\n",
    "        \n",
    "        \n",
    "    def add_placeholders(self):\n",
    "        self.x1 = tf.compat.v1.placeholder(tf.int32, shape = [None, None], name = \"x1\")\n",
    "        self.x1_f = tf.compat.v1.placeholder(tf.float32, shape = [None, None], name = \"x1_f\")\n",
    "        self.x2 = tf.compat.v1.placeholder(tf.int32, shape = [None, None], name = \"x2\")\n",
    "        self.x1_length = tf.compat.v1.placeholder(tf.int32, shape = [None], name = \"x1_length\")\n",
    "        self.x2_length = tf.compat.v1.placeholder(tf.int32, shape = [None], name = \"x2_length\")\n",
    "        \n",
    "        self.target_s = tf.compat.v1.placeholder(tf.int32, shape = [None], name = \"target_s\")\n",
    "        self.target_e = tf.compat.v1.placeholder(tf.int32, shape = [None], name = \"target_e\")\n",
    "        \n",
    "        self.dropout_pl = tf.compat.v1.placeholder(dtype=tf.float32, shape=[], name=\"dropout\")\n",
    "        self.lr_pl = tf.compat.v1.placeholder(dtype=tf.float32, shape=[], name=\"lr\")\n",
    "        \n",
    "        \n",
    "    def lookup_layer_op(self):\n",
    "        with tf.compat.v1.variable_scope(\"words\"):\n",
    "            _word_embeddings = tf.Variable(self.embeddings,\n",
    "                                           dtype=tf.float32,\n",
    "                                           trainable=self.update_embedding,\n",
    "                                           name=\"_word_embeddings\")\n",
    "            x1_emb = tf.nn.embedding_lookup(params=_word_embeddings,\n",
    "                                                     ids=self.x1,\n",
    "                                                     name=\"x1_emb\")\n",
    "            x2_emb = tf.nn.embedding_lookup(params=_word_embeddings,\n",
    "                                                     ids=self.x2,\n",
    "                                                     name=\"x2_emb\")\n",
    "        self.x1_emb =  tf.nn.dropout(x1_emb, self.dropout_pl)\n",
    "        self.x2_emb =  tf.nn.dropout(x2_emb, self.dropout_pl)\n",
    "        \n",
    "    \n",
    "    def RNN_layer_op(self, use_avg_weight = \"attn\"):\n",
    "        assert use_avg_weight in [\"attn\", \"unif\"]\n",
    "        \n",
    "        drnn_input = [self.x1_emb]\n",
    "        print(self.x1_emb.shape)\n",
    "        \n",
    "        def produce_attn_layer(layer_name, query_input, value_input, filters = 100, kernel_size = 4):\n",
    "            with tf.compat.v1.variable_scope(layer_name):\n",
    "                cnn_layer = tf.keras.layers.Conv1D(\n",
    "                    filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    # Use 'same' padding so outputs have the same shape as inputs.\n",
    "                    padding='same')\n",
    "                # Query encoding of shape [batch_size, Tq, filters].\n",
    "                query_seq_encoding = cnn_layer(query_input)\n",
    "                # Value encoding of shape [batch_size, Tv, filters].\n",
    "                value_seq_encoding = cnn_layer(value_input)\n",
    "\n",
    "                # Query-value attention of shape [batch_size, Tq, filters].\n",
    "                query_value_attention_seq = tf.keras.layers.Attention()(\n",
    "                    [query_seq_encoding, value_seq_encoding])\n",
    "                \n",
    "                return query_value_attention_seq\n",
    "        \n",
    "\n",
    "        x2_weighted_emb = produce_attn_layer(\"x2_weighted_emb\", self.x1_emb, self.x2_emb)\n",
    "        drnn_input.append(x2_weighted_emb)\n",
    "\n",
    "        \n",
    "        x1_f_reshape = tf.reshape(self.x1_f, [-1, tf.shape(self.x1_emb)[1], self.num_features])\n",
    "        drnn_input.append(x1_f_reshape)\n",
    "        \n",
    "        #### [B, N1, emb_dim + filters + feat_dim]\n",
    "        before_rnn_input = tf.concat(drnn_input, axis = -1)\n",
    "        \n",
    "        def produce_birnn_layer(layer_name, hidden_size = 128):\n",
    "            with tf.compat.v1.variable_scope(layer_name):\n",
    "                forward_layer = LSTM(hidden_size, return_sequences=True)\n",
    "                backward_layer = LSTM(hidden_size, return_sequences=True, go_backwards=True)\n",
    "                return Bidirectional(forward_layer, backward_layer=backward_layer)\n",
    "        \n",
    "        \n",
    "        bi_stack_lstm_with_mask_doc = tf.keras.Sequential([\n",
    "                tf.keras.layers.Masking(mask_value=0.),\n",
    "                produce_birnn_layer(\"doc-bi-stack-lstm\")\n",
    "                ])\n",
    "        inputs = before_rnn_input\n",
    "        mask = tf.expand_dims(tf.sequence_mask(self.x1_length, tf.shape(self.x1_emb)[1], dtype = tf.float32), axis = -1)\n",
    "        #### [B, N1, hidden-dim]\n",
    "        doc_hiddens = bi_stack_lstm_with_mask_doc(inputs * mask)\n",
    "        \n",
    "        bi_stack_lstm_with_mask_quest = tf.keras.Sequential([\n",
    "                tf.keras.layers.Masking(mask_value=0.),\n",
    "                produce_birnn_layer(\"quest-bi-stack-lstm\")\n",
    "                ])\n",
    "        inputs = self.x2_emb\n",
    "        mask = tf.expand_dims(tf.sequence_mask(self.x2_length, tf.shape(self.x2_emb)[1], dtype = tf.float32), axis = -1)\n",
    "        #### [B, N2, hidden-dim]\n",
    "        quest_hiddens = bi_stack_lstm_with_mask_quest(inputs * mask)\n",
    "        \n",
    "        def uniform_weights(x, x_length):\n",
    "            inputs = tf.ones(tf.shape(x)[:2])\n",
    "            mask = tf.sequence_mask(x_length, tf.shape(inputs)[1], dtype = tf.float32)\n",
    "            mask_output = tf.keras.layers.Masking(mask_value=0.)(inputs * mask)\n",
    "            mask_output = mask_output / tf.expand_dims(tf.reduce_sum(mask_output, axis = 1), \n",
    "                                                       len(mask_output.shape) - 1)\n",
    "            \n",
    "            return mask_output\n",
    "        \n",
    "        \n",
    "        #### [B, hidden-dim]  \n",
    "        if use_avg_weight == \"unif\":\n",
    "            #### [B, N2]\n",
    "            uniform_q_merged_weights = uniform_weights(quest_hiddens, self.x2_length)\n",
    "            question_hidden = tf.reduce_sum(quest_hiddens * tf.expand_dims(uniform_q_merged_weights, axis = 2), axis = 1)\n",
    "        else:\n",
    "            ##### 20 == hidden_size * 2\n",
    "            before_softmax = tf.reshape(tf.keras.layers.Dense(1)(\n",
    "                tf.reshape(quest_hiddens, [-1, 128 * 2])\n",
    "                ), \n",
    "                                    [tf.shape(quest_hiddens)[0], \n",
    "                                     tf.shape(quest_hiddens)[1]])\n",
    "        \n",
    "\n",
    "            inputs = before_softmax\n",
    "            mask = tf.sequence_mask(self.x2_length, tf.shape(self.x2_emb)[1], dtype = tf.float32)\n",
    "            #### [B, N2]\n",
    "            attn_q_merged_weights = tf.keras.Sequential([tf.keras.layers.Masking(mask_value=0.), \n",
    "                                                 tf.keras.layers.Softmax(axis = 1)])(inputs * mask)\n",
    "            question_hidden = tf.reduce_sum(quest_hiddens * tf.expand_dims(attn_q_merged_weights, axis = 2), axis = 1)\n",
    "        \n",
    "        \n",
    "        question_hidden_tiled = tf.tile(tf.expand_dims(question_hidden, axis = 1),\n",
    "        [1, tf.shape(doc_hiddens)[1], 1])\n",
    "        #### [B, N1]\n",
    "        #start_score_before = tf.squeeze(produce_attn_layer(\"start_score_before\", doc_hiddens, question_hidden_tiled, 1), -1)\n",
    "        start_score_before = produce_attn_layer(\"start_score_before\", doc_hiddens, question_hidden_tiled, 100)\n",
    "        start_score_before = tf.reshape(tf.keras.layers.Dense(1)(\n",
    "                tf.reshape(start_score_before, [-1, 100])\n",
    "                ), \n",
    "                                    [tf.shape(start_score_before)[0], \n",
    "                                     tf.shape(start_score_before)[1]])\n",
    "        \n",
    "        \n",
    "        inputs = start_score_before\n",
    "        mask = tf.sequence_mask(self.x1_length, tf.shape(self.x1_emb)[1], dtype = tf.float32)\n",
    "        #### [B, N1]\n",
    "        start_score_before_softmax = tf.keras.Sequential([tf.keras.layers.Masking(mask_value=0.)])(inputs * mask)\n",
    "        \n",
    "        #end_score_before = tf.squeeze(produce_attn_layer(\"end_score_before\", doc_hiddens, question_hidden_tiled, 1), -1)\n",
    "        end_score_before = produce_attn_layer(\"end_score_before\", doc_hiddens, question_hidden_tiled, 100)\n",
    "        end_score_before = tf.reshape(tf.keras.layers.Dense(1)(\n",
    "                tf.reshape(end_score_before, [-1, 100])\n",
    "                ), \n",
    "                                    [tf.shape(end_score_before)[0], \n",
    "                                     tf.shape(end_score_before)[1]])\n",
    "        \n",
    "        inputs = end_score_before\n",
    "        mask = tf.sequence_mask(self.x1_length, tf.shape(self.x1_emb)[1], dtype = tf.float32)\n",
    "        #### [B, N1]\n",
    "        end_score_before_softmax = tf.keras.Sequential([tf.keras.layers.Masking(mask_value=0.)])(inputs * mask)\n",
    "        \n",
    "        self.start_score_before_softmax, self.end_score_before_softmax = start_score_before_softmax, end_score_before_softmax\n",
    "    \n",
    "    def loss_op(self):\n",
    "        self.start_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.target_s, logits=self.start_score_before_softmax)\n",
    "        self.end_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.target_e, logits=self.end_score_before_softmax)\n",
    "        self.loss = tf.reduce_mean(self.start_loss) + tf.reduce_mean(self.end_loss)\n",
    "        #self.loss = tf.reduce_mean(self.end_loss)\n",
    "    \n",
    "    def softmax_pred_op(self):\n",
    "        self.labels_softmax_start = tf.argmax(self.start_score_before_softmax, axis=-1)\n",
    "        self.labels_softmax_start = tf.cast(self.labels_softmax_start, tf.int32)\n",
    "        \n",
    "        self.labels_softmax_end = tf.argmax(self.end_score_before_softmax, axis=-1)\n",
    "        self.labels_softmax_end = tf.cast(self.labels_softmax_end, tf.int32)\n",
    "    \n",
    "    def trainstep_op(self):\n",
    "        with tf.compat.v1.variable_scope(\"train_step\"):\n",
    "            self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            if self.optimizer == 'Adam':\n",
    "                optim = tf.compat.v1.train.AdamOptimizer(learning_rate=self.lr_pl)\n",
    "            elif self.optimizer == 'Adadelta':\n",
    "                optim = tf.compat.v1.train.AdadeltaOptimizer(learning_rate=self.lr_pl)\n",
    "            elif self.optimizer == 'Adagrad':\n",
    "                optim = tf.compat.v1.train.AdagradOptimizer(learning_rate=self.lr_pl)\n",
    "            elif self.optimizer == 'RMSProp':\n",
    "                optim = tf.compat.v1.train.RMSPropOptimizer(learning_rate=self.lr_pl)\n",
    "            elif self.optimizer == 'Momentum':\n",
    "                optim = tf.compat.v1.train.MomentumOptimizer(learning_rate=self.lr_pl, momentum=0.9)\n",
    "            elif self.optimizer == 'SGD':\n",
    "                optim = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=self.lr_pl)\n",
    "            else:\n",
    "                optim = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=self.lr_pl)\n",
    "                \n",
    "            self.train_op = optim.minimize(self.loss)\n",
    "            \n",
    "            '''\n",
    "            grads_and_vars = optim.compute_gradients(self.loss)\n",
    "            grads_and_vars_clip = [[tf.clip_by_value(g, -self.clip_grad, self.clip_grad), v] for g, v in grads_and_vars]\n",
    "            self.train_op = optim.apply_gradients(grads_and_vars_clip, global_step=self.global_step)\n",
    "            '''\n",
    "            \n",
    "    def init_op(self):\n",
    "        self.init_op = tf.compat.v1.global_variables_initializer()\n",
    "        \n",
    "    def add_summary(self, sess):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sess:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.merged = tf.compat.v1.summary.merge_all()\n",
    "        self.file_writer = tf.compat.v1.summary.FileWriter(self.summary_path, sess.graph)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "len(word_dict)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "EMB_DIM, VOC_SIZE, F_DIM = 300, len(word_dict), len(feature_dict)\n",
    "embedding_path = \"glove_embedding.pkl\"\n",
    "assert os.path.exists(embedding_path)\n",
    "import pickle as pkl\n",
    "with open(embedding_path, \"rb\") as f:\n",
    "    embeddings_np = pkl.load(f)\n",
    "#embeddings_np = np.random.random([VOC_SIZE, EMB_DIM])\n",
    "assert embeddings_np.shape == (VOC_SIZE, EMB_DIM)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "args_dict\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "rnnReader_ext = RnnDocReader_tf(embeddings_np, num_features=F_DIM)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "len(dev_exs)\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "#train_gen = batch_samples_gen(train_exs[:1000], epoch_num=1000)\n",
    "train_gen = batch_samples_gen(train_exs, epoch_num=1000, batch_size=32)\n",
    "from drqa.reader.utils import load_data\n",
    "from collections import namedtuple\n",
    "load_data_args = namedtuple(\"load_data_args\", [\"uncased_question\", \"uncased_doc\"])\n",
    "load_data_args.uncased_doc = False\n",
    "load_data_args.uncased_question = False\n",
    "#json_file_name = \"/home/svjack/temp_dir/DrQA/DrQA/data/datasets/SQuAD-v1.1-dev-processed-corenlp.txt\" \n",
    "#json_file_name = \"notebook_dumps/SQuAD-v1.1-dev-processed-corenlp.txt\"\n",
    "json_file_name = \"../data/datasets/SQuAD-v1.1-dev-processed-corenlp.txt\"\n",
    "dev_exs = load_data(load_data_args ,json_file_name, skip_no_answer=True)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "len(dev_exs)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "#mode = \"re-train\"\n",
    "mode = \"train\"\n",
    "assert mode in [\"train\", \"re-train\"]\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "if mode == \"train\":\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    model_path = \"save\"\n",
    "    import shutil, os\n",
    "    if os.path.exists(model_path):\n",
    "        shutil.rmtree(model_path)\n",
    "    os.mkdir(model_path)\n",
    "    model_path = \"save/model\"\n",
    "    os.mkdir(model_path)\n",
    "    \n",
    "    saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables(), max_to_keep = 5)\n",
    "else:\n",
    "    assert mode == \"re-train\"\n",
    "    import os, shutil\n",
    "    assert os.path.exists(\"save\")\n",
    "    model_path = \"save\"\n",
    "    latest_path = tf.compat.v1.train.latest_checkpoint(model_path)\n",
    "    \n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "    saver.restore(sess, latest_path)\n",
    "model_path  = \"save/model\"\n",
    "\n",
    "def retrieve_conclusion(model ,ele, with_train = True):\n",
    "    x1, x1_f, x1_length, x2, x2_length, target_s, target_e = ele\n",
    "    if with_train:\n",
    "        dropout_pl_np = 0.6\n",
    "    else:\n",
    "        dropout_pl_np = 0.0\n",
    "    \n",
    "    lr_pl_np = 0.0001\n",
    "    feed_dict = {\n",
    "    model.x1:x1, \n",
    "    model.x2:x2, \n",
    "    model.x1_f:x1_f.reshape([len(x1), -1]), \n",
    "    model.x1_length:x1_length, \n",
    "    model.x2_length:x2_length,\n",
    "    model.target_s:target_s,\n",
    "    model.target_e:target_e,\n",
    "    model.dropout_pl:dropout_pl_np,\n",
    "    model.lr_pl:lr_pl_np\n",
    "    }\n",
    "    \n",
    "    if with_train:\n",
    "        _, loss, labels_softmax_start, labels_softmax_end = sess.run(\n",
    "        [model.train_op, \n",
    "         model.loss, \n",
    "         model.labels_softmax_start,\n",
    "        model.labels_softmax_end,\n",
    "        ],        \n",
    "    feed_dict = feed_dict)\n",
    "    else:\n",
    "        loss, labels_softmax_start, labels_softmax_end = sess.run(\n",
    "        [ \n",
    "         model.loss, \n",
    "         model.labels_softmax_start,\n",
    "        model.labels_softmax_end,\n",
    "        ],        \n",
    "    feed_dict = feed_dict)\n",
    "        \n",
    "    start_score = np.asarray(labels_softmax_start == target_s, dtype = int).sum() / len(labels_softmax_start)\n",
    "    end_score = np.asarray(labels_softmax_end == target_e, dtype = int).sum() / len(labels_softmax_end)\n",
    "    \n",
    "    return model ,loss, start_score, end_score \n",
    "\n",
    "def retrieve_conclusion_with_output(model ,ele, with_train = True):\n",
    "    x1, x1_f, x1_length, x2, x2_length, target_s, target_e = ele\n",
    "    if with_train:\n",
    "        dropout_pl_np = 0.6\n",
    "    else:\n",
    "        dropout_pl_np = 0.0\n",
    "    \n",
    "    lr_pl_np = 0.0001\n",
    "    feed_dict = {\n",
    "    model.x1:x1, \n",
    "    model.x2:x2, \n",
    "    model.x1_f:x1_f.reshape([len(x1), -1]), \n",
    "    model.x1_length:x1_length, \n",
    "    model.x2_length:x2_length,\n",
    "    model.target_s:target_s,\n",
    "    model.target_e:target_e,\n",
    "    model.dropout_pl:dropout_pl_np,\n",
    "    model.lr_pl:lr_pl_np\n",
    "    }\n",
    "    \n",
    "    if with_train:\n",
    "        _, loss, labels_softmax_start, labels_softmax_end = sess.run(\n",
    "        [model.train_op, \n",
    "         model.loss, \n",
    "         model.labels_softmax_start,\n",
    "        model.labels_softmax_end,\n",
    "        ],        \n",
    "    feed_dict = feed_dict)\n",
    "    else:\n",
    "        loss, labels_softmax_start, labels_softmax_end = sess.run(\n",
    "        [ \n",
    "         model.loss, \n",
    "         model.labels_softmax_start,\n",
    "        model.labels_softmax_end,\n",
    "        ],        \n",
    "    feed_dict = feed_dict)\n",
    "        \n",
    "    start_score = np.asarray(labels_softmax_start == target_s, dtype = int).sum() / len(labels_softmax_start)\n",
    "    end_score = np.asarray(labels_softmax_end == target_e, dtype = int).sum() / len(labels_softmax_end)\n",
    "    score = (np.asarray(labels_softmax_start == target_s, dtype = int) * np.asarray(labels_softmax_end == target_e, dtype = int)).sum() / len(labels_softmax_end)\n",
    "    \n",
    "    return model ,loss, start_score, end_score, score, labels_softmax_start, labels_softmax_end\n",
    "\n",
    "from shutil import copyfile \n",
    "log_file = \"log.txt\"\n",
    "f = open(log_file, \"w\")\n",
    "f.close()\n",
    "\n",
    "best_dev_loss = np.inf\n",
    "best_start, best_end = 0, 0\n",
    "pred_save_format = \"pred_start_{}_end_{}.pkl\"\n",
    "\n",
    "for idx ,ele in enumerate(train_gen):\n",
    "    rnnReader_ext ,train_loss, train_start_score, train_end_score = retrieve_conclusion(rnnReader_ext, ele, with_train=True)\n",
    "    \n",
    "    #if (idx + 1) % 100 == 0:\n",
    "    if (idx) % 100 == 0:\n",
    "         f = open(log_file, \"r\")\n",
    "         have_text = f.read() + \"\\n\"\n",
    "         f.close()\n",
    "         have_text += \"idx {} train_loss: {}, start: {}, end: {}\\n dev_loss: {} start: {}, end: {}\".format(idx, \n",
    "                train_loss, train_start_score, train_end_score, None, None, None\n",
    "                                                                                                  )\n",
    "         f = open(log_file, \"w\")\n",
    "         f.write(have_text)\n",
    "         f.close()  \n",
    "         copyfile(log_file, log_file.replace(\".txt\", \"_cp.txt\"))                                               \n",
    "         print(\"idx {} train_loss: {}, start: {}, end: {}\\n dev_loss: {} start: {}, end: {}\".format(idx, \n",
    "                train_loss, train_start_score, train_end_score, None, None, None\n",
    "                                                                                                  ))\n",
    "    \n",
    "    #if (idx + 1) % 1000 == 0:\n",
    "    if (idx) % 3000 == 0:\n",
    "        dev_gen = batch_samples_gen(dev_exs[:100000], epoch_num=1, batch_size=32)\n",
    "        dev_loss_list, dev_s_list, dev_e_list = [], [], []\n",
    "        start_pred_list, end_pred_list = [], []\n",
    "        dev_list = []\n",
    "        for d_ele in dev_gen:\n",
    "            #rnnReader_ext ,dev_loss, dev_start_score, dev_end_score = retrieve_conclusion(rnnReader_ext, d_ele, with_train=False)\n",
    "            rnnReader_ext ,dev_loss, dev_start_score, dev_end_score, dev_score, start_pred, end_pred = retrieve_conclusion_with_output(rnnReader_ext, d_ele, with_train=False)\n",
    "            dev_loss_list.append(dev_loss)\n",
    "            dev_s_list.append(dev_start_score)\n",
    "            dev_e_list.append(dev_end_score)\n",
    "            \n",
    "            dev_list.append(dev_score)\n",
    "            \n",
    "            start_pred_list.extend(start_pred.tolist())\n",
    "            end_pred_list.extend(end_pred.tolist())\n",
    "            \n",
    "        dev_loss, dev_s, dev_e, dev_ = map(lambda l: pd.Series(l).mean(), [dev_loss_list, dev_s_list, dev_e_list, dev_list])\n",
    "        #print(idx, train_loss, start_score, end_score)\n",
    "        f = open(log_file, \"r\")\n",
    "        have_text = f.read() + \"\\n\"\n",
    "        f.close()\n",
    "        have_text += \"idx {} train_loss: {}, start: {}, end: {}\\n dev_loss: {} start: {}, end: {}, score: {}\".format(idx, \n",
    "                train_loss, train_start_score, train_end_score, dev_loss, dev_s, dev_e, dev_\n",
    "                                                                                                  )\n",
    "        with open(pred_save_format.format(dev_s, dev_e), \"wb\") as f:\n",
    "            pkl.dump({\n",
    "                    \"start\":start_pred_list,\n",
    "                    \"end\":end_pred_list\n",
    "                    }, f)\n",
    "        \n",
    "        f = open(log_file, \"w\")\n",
    "        f.write(have_text)\n",
    "        f.close()\n",
    "        copyfile(log_file, log_file.replace(\".txt\", \"_cp.txt\"))\n",
    "        #if dev_loss < best_dev_loss:\n",
    "        if best_start < dev_s or best_end < dev_e:\n",
    "            #best_dev_loss = dev_loss\n",
    "            best_start = dev_s\n",
    "            best_end = dev_e\n",
    "            saver.save(sess, model_path, idx)  \n",
    "        print(\"idx {} train_loss: {}, start: {}, end: {}\\n dev_loss: {} start: {}, end: {}, score: {}\".format(idx, \n",
    "                train_loss, train_start_score, train_end_score, dev_loss, dev_s, dev_e, dev_\n",
    "                                                                                                  ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
